{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there is a StudioHaptics wrapper in Terra Creator Studio. It provides static methods to play various haptic feedback patterns using the Lofelt Nice Vibrations library. The available methods include PlayHapticSelection, PlayHapticSuccess, PlayHapticWarning, PlayHapticFailure, PlayHapticLightImpact, PlayHapticMediumImpact, PlayHapticHeavyImpact, PlayHapticRigidImpact, and PlayHapticSoftImpact. Each method triggers a different type of haptic feedback. If you need assistance with any specific method usage or implementation, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")  # GitHub token\n",
    "\n",
    "# Initialize ChatOpenAI with the model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# GitHub repository details\n",
    "repo_owner = \"ashwinknan\"\n",
    "repo_name = \"testterra\"\n",
    "branch_name = \"main\"  # or the branch you're using\n",
    "\n",
    "# Function to fetch markdown files from GitHub\n",
    "def fetch_markdown_files_from_github(repo_owner, repo_name, branch_name, token):\n",
    "    api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents?ref={branch_name}\"\n",
    "    headers = {\n",
    "        'Authorization': f'token {token}'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        return []\n",
    "    except Exception as err:\n",
    "        print(f\"Other error occurred: {err}\")\n",
    "        return []\n",
    "\n",
    "    file_list = response.json()\n",
    "    \n",
    "    markdown_files = []\n",
    "    for file in file_list:\n",
    "        if file['name'].endswith('.md'):\n",
    "            file_url = file['download_url']\n",
    "            try:\n",
    "                file_content = requests.get(file_url, headers=headers).text\n",
    "                markdown_files.append(file_content)\n",
    "            except requests.exceptions.HTTPError as http_err:\n",
    "                print(f\"HTTP error occurred while fetching file {file_url}: {http_err}\")\n",
    "            except Exception as err:\n",
    "                print(f\"Other error occurred while fetching file {file_url}: {err}\")\n",
    "    \n",
    "    return markdown_files\n",
    "\n",
    "# Load and process markdown files from GitHub\n",
    "def load_markdown_files():\n",
    "    all_documents = []\n",
    "    markdown_files = fetch_markdown_files_from_github(repo_owner, repo_name, branch_name, github_token)\n",
    "\n",
    "    for file_content in markdown_files:\n",
    "        # Write the content to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".md\") as temp_file:\n",
    "            temp_file.write(file_content.encode('utf-8'))\n",
    "            temp_file_path = temp_file.name\n",
    "            \n",
    "            # Load the markdown file\n",
    "            loader = UnstructuredMarkdownLoader(temp_file_path)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Set metadata for each document using the original file name\n",
    "            for doc in documents:\n",
    "                doc.metadata['source'] = os.path.basename(temp_file_path)  # Use the file name instead of the path\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "        \n",
    "        # Optionally, delete the temporary file after loading\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "# Clear and reload documents\n",
    "all_documents = load_markdown_files()\n",
    "\n",
    "# Ensure we have loaded documents\n",
    "assert len(all_documents) > 0\n",
    "\n",
    "# Split the documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=500, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Create a new vector store from the documents\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# Define the refined system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks related to game development using the Terra Creator Studio engine. \"\n",
    "    \"Your responsibilities include providing complete T# function implementations when requested, ensuring the code is self-contained and ready for use. \"\n",
    "    \"Prioritize modifying existing wrapper functions or combining them over creating new ones unless necessary. \"\n",
    "    \"Most syntax is similar to C#, but be aware of differences, especially regarding access wrappers and methods. \"\n",
    "    \"Refer to the 'T# Don'ts' section to avoid common pitfalls. \"\n",
    "    \"If a script is requested, provide complete T# function code that can be directly copied into Terra Studio. \"\n",
    "    \"Ensure consistency in responses; similar questions should yield similar answers. \"\n",
    "    \"Always refer to the provided context below and search relevant portions for feature-related questions. \"\n",
    "    \"Double-check the context document for accuracy, as verifying information is more important than speed. \"\n",
    "    \"If you cannot find an answer in the T# documentation, state that you don't know. \"\n",
    "    \"Your answers should be clear, concise, and suitable for novice developers. \"\n",
    "    \"Always include the source of the information used in your response, and ensure the sources are accurate. \"\n",
    "    \"If you reference a source, please specify its location.\"\n",
    "    \"\\n\\n{context}\\n\\nSources:\\n{sources}\"\n",
    ")\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the question-answer chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Store for chat history\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap the RAG chain with message history management\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "session_id = \"abc1112\"  # Unique session ID for the conversation\n",
    "\n",
    "# Test the retrieval-augmented generation chain\n",
    "input_question = \"Isn't there a StudioHaptics wrapper present. Check please\"\n",
    "retrieved_docs = retriever.invoke(input_question)\n",
    "\n",
    "# Check if any documents were retrieved\n",
    "if not retrieved_docs:\n",
    "    print(\"No documents retrieved. Please check your query.\")\n",
    "else:\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    # Create a more informative sources output\n",
    "    sources = \"\\n\".join(f\"- Source: {doc.metadata['source']}\\n  Content: {doc.page_content[:300]}...\" for doc in retrieved_docs)  # Show first 300 characters\n",
    "\n",
    "    # Retrieve the chat history for the session\n",
    "    chat_history = get_session_history(session_id)\n",
    "\n",
    "    # Include chat history in the input\n",
    "    input_with_history = \"\\n\".join([f\"User: {msg['input']}\\nAI: {msg['answer']}\" for msg in chat_history.messages]) + f\"\\nUser: {input_question}\"\n",
    "\n",
    "    formatted_prompt = system_prompt.format(context=context, sources=sources)\n",
    "\n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": input_question, \"context\": context, \"sources\": sources},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    # Store the new question and answer in the chat history\n",
    "    chat_history.messages.append({\"input\": input_question, \"answer\": response[\"answer\"]})\n",
    "\n",
    "    print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
